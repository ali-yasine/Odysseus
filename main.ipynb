{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "import torch.optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_16708\\3569919354.py:10: DtypeWarning: Columns (8,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/HIGGS_train.csv', header=None, names=cols)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600000 entries, 0 to 599999\n",
      "Data columns (total 29 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   class_label               600000 non-null  float64\n",
      " 1   lepton_pt                 600000 non-null  float64\n",
      " 2   lepton_eta                600000 non-null  float64\n",
      " 3   lepton_phi                600000 non-null  float64\n",
      " 4   missing_energy_magnitude  600000 non-null  float64\n",
      " 5   missing_energy_phi        600000 non-null  float64\n",
      " 6   jet_1_pt                  600000 non-null  float64\n",
      " 7   jet_1_eta                 600000 non-null  float64\n",
      " 8   jet_1_phi                 600000 non-null  object \n",
      " 9   jet_1_b-tag               600000 non-null  float64\n",
      " 10  jet_2_pt                  600000 non-null  float64\n",
      " 11  jet_2_eta                 600000 non-null  float64\n",
      " 12  jet_2_phi                 600000 non-null  float64\n",
      " 13  jet_2_b-tag               600000 non-null  float64\n",
      " 14  jet_3_pt                  600000 non-null  float64\n",
      " 15  jet_3_eta                 600000 non-null  float64\n",
      " 16  jet_3_phi                 600000 non-null  float64\n",
      " 17  jet_3_b-tag               599999 non-null  float64\n",
      " 18  jet_4_pt                  600000 non-null  float64\n",
      " 19  jet_4_eta                 600000 non-null  float64\n",
      " 20  jet_4_phi                 600000 non-null  float64\n",
      " 21  jet_4_b-tag               600000 non-null  object \n",
      " 22  m_jj                      600000 non-null  float64\n",
      " 23  m_jjj                     600000 non-null  float64\n",
      " 24  m_lv                      600000 non-null  float64\n",
      " 25  m_jlv                     600000 non-null  float64\n",
      " 26  m_bb                      600000 non-null  float64\n",
      " 27  m_wbb                     600000 non-null  float64\n",
      " 28  m_wwbb                    600000 non-null  float64\n",
      "dtypes: float64(27), object(2)\n",
      "memory usage: 132.8+ MB\n"
     ]
    }
   ],
   "source": [
    "cols = [\n",
    "    'class_label', 'lepton_pt', 'lepton_eta', 'lepton_phi', 'missing_energy_magnitude',\n",
    "    'missing_energy_phi', 'jet_1_pt', 'jet_1_eta', 'jet_1_phi', 'jet_1_b-tag',\n",
    "    'jet_2_pt', 'jet_2_eta', 'jet_2_phi', 'jet_2_b-tag', 'jet_3_pt',\n",
    "    'jet_3_eta', 'jet_3_phi', 'jet_3_b-tag', 'jet_4_pt', 'jet_4_eta',\n",
    "    'jet_4_phi', 'jet_4_b-tag', 'm_jj', 'm_jjj', 'm_lv', 'm_jlv',\n",
    "    'm_bb', 'm_wbb', 'm_wwbb'\n",
    "]\n",
    "\n",
    "df = pd.read_csv('data/HIGGS_train.csv', header=None, names=cols)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 599996 entries, 0 to 599999\n",
      "Data columns (total 29 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   class_label               599996 non-null  float64\n",
      " 1   lepton_pt                 599996 non-null  float64\n",
      " 2   lepton_eta                599996 non-null  float64\n",
      " 3   lepton_phi                599996 non-null  float64\n",
      " 4   missing_energy_magnitude  599996 non-null  float64\n",
      " 5   missing_energy_phi        599996 non-null  float64\n",
      " 6   jet_1_pt                  599996 non-null  float64\n",
      " 7   jet_1_eta                 599996 non-null  float64\n",
      " 8   jet_1_phi                 599996 non-null  float64\n",
      " 9   jet_1_b-tag               599996 non-null  float64\n",
      " 10  jet_2_pt                  599996 non-null  float64\n",
      " 11  jet_2_eta                 599996 non-null  float64\n",
      " 12  jet_2_phi                 599996 non-null  float64\n",
      " 13  jet_2_b-tag               599996 non-null  float64\n",
      " 14  jet_3_pt                  599996 non-null  float64\n",
      " 15  jet_3_eta                 599996 non-null  float64\n",
      " 16  jet_3_phi                 599996 non-null  float64\n",
      " 17  jet_3_b-tag               599996 non-null  float64\n",
      " 18  jet_4_pt                  599996 non-null  float64\n",
      " 19  jet_4_eta                 599996 non-null  float64\n",
      " 20  jet_4_phi                 599996 non-null  float64\n",
      " 21  jet_4_b-tag               599996 non-null  float64\n",
      " 22  m_jj                      599996 non-null  float64\n",
      " 23  m_jjj                     599996 non-null  float64\n",
      " 24  m_lv                      599996 non-null  float64\n",
      " 25  m_jlv                     599996 non-null  float64\n",
      " 26  m_bb                      599996 non-null  float64\n",
      " 27  m_wbb                     599996 non-null  float64\n",
      " 28  m_wwbb                    599996 non-null  float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 137.3 MB\n"
     ]
    }
   ],
   "source": [
    "#count the number of missing values in each column\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df.isnull().sum()\n",
    "#remove rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:].values \n",
    "y = df.iloc[:, 0].values\n",
    "\n",
    "#split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=313)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=313)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=313).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7323525816953522\n",
      "0.7580958911611273\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "print(model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7328596269979277\n",
      "0.7580958911611273\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.20806 | val_0_unsup_loss_numpy: 0.8765199780464172|  0:00:42s\n",
      "epoch 1  | loss: 0.94531 | val_0_unsup_loss_numpy: 0.874809980392456|  0:01:33s\n",
      "epoch 2  | loss: 0.93886 | val_0_unsup_loss_numpy: 0.8457000255584717|  0:02:03s\n",
      "epoch 3  | loss: 0.93382 | val_0_unsup_loss_numpy: 0.8651400208473206|  0:02:33s\n",
      "epoch 4  | loss: 0.9317  | val_0_unsup_loss_numpy: 0.8500400185585022|  0:03:04s\n",
      "epoch 5  | loss: 0.92804 | val_0_unsup_loss_numpy: 0.8390399813652039|  0:03:33s\n",
      "epoch 6  | loss: 0.92585 | val_0_unsup_loss_numpy: 0.8735700249671936|  0:04:03s\n",
      "epoch 7  | loss: 0.92717 | val_0_unsup_loss_numpy: 0.8381699919700623|  0:04:33s\n",
      "epoch 8  | loss: 0.92484 | val_0_unsup_loss_numpy: 0.8323100209236145|  0:05:00s\n",
      "epoch 9  | loss: 0.92409 | val_0_unsup_loss_numpy: 0.8367400169372559|  0:05:30s\n",
      "epoch 10 | loss: 0.92414 | val_0_unsup_loss_numpy: 0.8262500166893005|  0:05:57s\n",
      "epoch 11 | loss: 0.92439 | val_0_unsup_loss_numpy: 0.8359900116920471|  0:06:26s\n",
      "epoch 12 | loss: 0.92449 | val_0_unsup_loss_numpy: 0.8330699801445007|  0:06:58s\n",
      "epoch 13 | loss: 0.92533 | val_0_unsup_loss_numpy: 0.8285800218582153|  0:07:28s\n",
      "epoch 14 | loss: 0.92273 | val_0_unsup_loss_numpy: 0.8304799795150757|  0:07:57s\n",
      "epoch 15 | loss: 0.92282 | val_0_unsup_loss_numpy: 0.8463699817657471|  0:08:42s\n",
      "epoch 16 | loss: 0.92326 | val_0_unsup_loss_numpy: 0.828540027141571|  0:09:19s\n",
      "epoch 17 | loss: 0.92329 | val_0_unsup_loss_numpy: 0.8354099988937378|  0:09:52s\n",
      "epoch 18 | loss: 0.92062 | val_0_unsup_loss_numpy: 0.8284199833869934|  0:10:19s\n",
      "epoch 19 | loss: 0.92181 | val_0_unsup_loss_numpy: 0.817359983921051|  0:10:50s\n",
      "epoch 20 | loss: 0.92255 | val_0_unsup_loss_numpy: 0.8201299905776978|  0:11:23s\n",
      "epoch 21 | loss: 0.92161 | val_0_unsup_loss_numpy: 0.8369399905204773|  0:11:52s\n",
      "epoch 22 | loss: 0.92171 | val_0_unsup_loss_numpy: 0.8232100009918213|  0:12:21s\n",
      "epoch 23 | loss: 0.92148 | val_0_unsup_loss_numpy: 0.8357499837875366|  0:12:50s\n",
      "epoch 24 | loss: 0.92031 | val_0_unsup_loss_numpy: 0.8259400129318237|  0:13:40s\n",
      "epoch 25 | loss: 0.92062 | val_0_unsup_loss_numpy: 0.8205599784851074|  0:14:26s\n",
      "epoch 26 | loss: 0.92    | val_0_unsup_loss_numpy: 0.824970006942749|  0:15:05s\n",
      "epoch 27 | loss: 0.92018 | val_0_unsup_loss_numpy: 0.8210600018501282|  0:15:34s\n",
      "epoch 28 | loss: 0.92106 | val_0_unsup_loss_numpy: 0.8174399733543396|  0:16:02s\n",
      "epoch 29 | loss: 0.92075 | val_0_unsup_loss_numpy: 0.8365799784660339|  0:16:30s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_unsup_loss_numpy = 0.817359983921051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "#tabnet pretraining\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='entmax' # \"sparsemax\"\n",
    ")\n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train=X_train,\n",
    "    eval_set=[X_val],\n",
    "    pretraining_ratio=0.8,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at ./test_pretrain.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./test_pretrain.zip'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsupervised_model.save_model('./test_pretrain')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'unsupervised_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m      1\u001b[0m clf \u001b[39m=\u001b[39m TabNetClassifier(\n\u001b[0;32m      2\u001b[0m     optimizer_fn\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam,\n\u001b[0;32m      3\u001b[0m     optimizer_params\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(lr\u001b[39m=\u001b[39m\u001b[39m2e-2\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m clf\u001b[39m.\u001b[39mfit(\n\u001b[0;32m     12\u001b[0m     X_train\u001b[39m=\u001b[39mX_train, y_train\u001b[39m=\u001b[39my_train,\n\u001b[0;32m     13\u001b[0m     eval_set\u001b[39m=\u001b[39m[(X_train, y_train), (X_val, y_val)],\n\u001b[0;32m     14\u001b[0m     eval_name\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     15\u001b[0m     eval_metric\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m],    \n\u001b[0;32m     16\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m,\n\u001b[0;32m     17\u001b[0m     virtual_batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m,\n\u001b[1;32m---> 18\u001b[0m     from_unsupervised\u001b[39m=\u001b[39munsupervised_model\n\u001b[0;32m     19\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unsupervised_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = TabNetClassifier(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params={\"step_size\":10, # how to use learning rate scheduler\n",
    "                      \"gamma\":0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='sparsemax', # This will be overwritten if using pretrain model\n",
    "    verbose=5\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['accuracy'],    \n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    from_unsupervised=un\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TabNetClassifier()  \n",
    "clf.fit(\n",
    "  X_train, y_train,\n",
    "  eval_set=[(X_val, y_val)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at ./tabnet_with_pretrain.zip\n"
     ]
    }
   ],
   "source": [
    "saving_path_name = \"./tabnet_with_pretrain\"\n",
    "saved_filepath = clf.save_model(saving_path_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabnet acc: 0.7394429969055384\n",
      "xgboost acc: 0.7328596269979277\n",
      "logistic regression acc: 0.6398035544641915\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 89.09007| val_0_unsup_loss_numpy: 1.776039958000183|  0:12:22s\n",
      "epoch 1  | loss: 2.88749 | val_0_unsup_loss_numpy: 1.5425200462341309|  0:34:06s\n",
      "epoch 2  | loss: 2.15739 | val_0_unsup_loss_numpy: 1.0488799810409546|  0:46:17s\n",
      "epoch 3  | loss: 1.16372 | val_0_unsup_loss_numpy: 0.9975799918174744|  0:58:30s\n",
      "epoch 4  | loss: 0.97847 | val_0_unsup_loss_numpy: 0.86735999584198|  1:10:34s\n"
     ]
    }
   ],
   "source": [
    "unsupervised_M = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=0.02),\n",
    "    verbose=True,\n",
    "    n_a=128,\n",
    "    n_d=128,\n",
    "    n_steps=20\n",
    ")\n",
    "\n",
    "unsupervised_M.fit(\n",
    "    X_train=X_train,\n",
    "    eval_set=[X_val],\n",
    "    pretraining_ratio=0.8,\n",
    "    batch_size=8192,\n",
    "    virtual_batch_size=256,\n",
    ")\n",
    "\n",
    "unsupervised_M.save_model('./test_pretrain_M')\n",
    "\n",
    "model_M = TabNetClassifier(n_d=96, n_a=32, lambda_sparse=0.000001,  n_steps=8, gamma=2.0, from_unsupervised=unsupervised_M)\n",
    "model_M.fit(X_train, y_train, eval_set=[(X_val, y_val)], batch_size=8192, virtual_batch_size=256, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --gres=gpu\n",
    "#SBATCH --mem=12000\n",
    "#SBATCH --time=0-03:00:00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabnet train acc: 0.7482315349871547\n",
      "tabnet acc: 0.7409415660174002\n",
      "xgboost acc: 0.7323525816953522\n",
      "xgboost auc: 0.8137412268368229\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print(f'tabnet train acc: {sum(clf.predict(X_train) == y_train) / len(y_train)}')\n",
    "print(f'tabnet acc: {sum(preds == y_test) / len(y_test)}')\n",
    "print(f'xgboost acc: {model.score(X_test, y_test)}')\n",
    "y_pred = model.predict_proba(X_test)[:,1]\n",
    "model.predict_proba(X_test)\n",
    "print(f'xgboost auc: {roc_auc_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
